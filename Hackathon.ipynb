{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c0a084eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install langchain\n",
    "#!pip3 install python-dotenv\n",
    "#!pip3 install openai\n",
    "#!pip3 install --upgrade --quiet  llama-cpp-python\n",
    "#!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b19e59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c19667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('github_issues.pkl')\n",
    "df = df.loc[df['pr'] == 'issue']\n",
    "df_rep = df.groupby('repo_name')\n",
    "df_rep.ngroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = df['repo_name'].unique().tolist()\n",
    "df0 = df[df['repo_name']==repo[0]]\n",
    "title0 = df0['title'].tolist()\n",
    "body0 = df0['body'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a81cb3",
   "metadata": {},
   "source": [
    "## Use langchain (currently not work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60d9fdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-FqOs48siHybSSi8qqFzGT3BlbkFJs7GuN1tEVUeENNyl68kC\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#import openai\n",
    "\n",
    "#from dotenv import load_dotenv, find_dotenv\n",
    "#_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "#print(openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f09c2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850dbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#template_string = \"\"\"Translate the text \\\n",
    "#that is delimited by triple backticks \\\n",
    "#into a style that is {style}. \\\n",
    "#text: ```{text}```\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3dd5e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.prompts import ChatPromptTemplate\n",
    "#prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "#print(prompt_template.messages[0].prompt)\n",
    "#print(prompt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0aa0a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.llms import LlamaCpp\n",
    "#from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "#from langchain_core.prompts import PromptTemplate\n",
    "#prompt = PromptTemplate.from_template(template_string)\n",
    "#callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = LlamaCpp(\n",
    "#    model_path=\"/Users/daniel/source/repos/hackathon/examples/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "#    temperature=0.75,\n",
    "#    max_tokens=2000,\n",
    "#    top_p=1,\n",
    "#    callback_manager=callback_manager,\n",
    "#    verbose=True,  # Verbose is required to pass to the callback manager\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e098940",
   "metadata": {},
   "source": [
    "## Use LLM directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1cc390a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4560.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '17', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "LLM = Llama(model_path=\"llama-2-7b-chat.Q5_K_M.gguf\", n_ctx=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b5166ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = f\"Given a customer request: title: {title0[0]} and content: {body0[0]},\\\n",
    "please evaluate the Net Promoter Score ranging from 0(detractor) to 10(promoter).\\\n",
    "The output should only be an integer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1dc8207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b2f652f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   87014.98 ms\n",
      "llama_print_timings:      sample time =      50.10 ms /   163 runs   (    0.31 ms per token,  3253.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14393.69 ms /    74 tokens (  194.51 ms per token,     5.14 tokens per second)\n",
      "llama_print_timings:        eval time =   53312.18 ms /   162 runs   (  329.09 ms per token,     3.04 tokens per second)\n",
      "llama_print_timings:       total time =   68495.06 ms /   236 tokens\n"
     ]
    }
   ],
   "source": [
    "customer_response = LLM(customer_messages, max_tokens=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8d185a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I understand that you are asking about the license of the samples provided in the repository. The Net Promoter Score (NPS) is a tool used to measure customer satisfaction and loyalty. It ranges from -100 (completely dissatisfied) to 100 (completely satisfied).\n",
      "Based on your request, I would give you an NPS score of 7 out of 10. The reason for this score is that while the repository provides information about the license of the samples, it would be more helpful if this information was included in the README file. This way, users can quickly find the necessary information without having to search through the entire repository.\n",
      "So, your NPS score is 7 out of 10.\n"
     ]
    }
   ],
   "source": [
    "print(customer_response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3792cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "License?\n",
      "What license do these samples fall under? It would be helpful to include this in the README, which would also be nice to have too! \n"
     ]
    }
   ],
   "source": [
    "print(title0[0])\n",
    "print(body0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d360a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b969c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
